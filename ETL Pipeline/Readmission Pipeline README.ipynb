{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fa91901-ac93-492c-936e-ba771fa48018",
   "metadata": {},
   "source": [
    "# ETL Pipeline for Silverstone Medical Center\n",
    "\n",
    "This repository contains the **ETL pipeline** for extracting, transforming, and loading (ETL) hospital patient data for Silverstone Medical Center. The pipeline extracts data from CSV files on a local folder, processes it, and loads it into a PostgreSQL database for analysis.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The ETL pipeline is implemented in Python and uses **pandas**, **SQLAlchemy**, and **PostgreSQL** for data extraction, transformation, and loading. The data processed includes patient admissions, readmissions, patients, and hospital capacity metrics.\n",
    "\n",
    "### Components of the Pipeline:\n",
    "\n",
    "1. **Extract**: Extracts data from CSV files stored in the `data/` folder.\n",
    "2. **Transform**: Cleans and processes the data, handling missing values, correcting invalid data, and preparing the data for loading.\n",
    "3. **Load**: Loads the transformed data into a PostgreSQL database, creating necessary tables.\n",
    "4. **Drop Tables**: Drops all existing tables in the PostgreSQL database before the new data is loaded to avoid conflicts.\n",
    "5. **Pipeline**: Calls all functions defined in the above files and executes them in order.\n",
    "6. **run_pipeline** Runs the pipeline\n",
    "\n",
    "### File Structure\n",
    "\n",
    "The pipeline is made up of several Python files located in the `/python_files` folder:\n",
    "\n",
    "\n",
    "## Detailed Explanation of Each Component\n",
    "\n",
    "### Extract\n",
    "\n",
    "The **extract.py** script loads data from the following CSV files in the `data/` folder:\n",
    "- `patients.csv`\n",
    "- `modified_admissions.csv`\n",
    "- `modified_readmissions.csv`\n",
    "- `modified_hospital_capacity.csv`\n",
    "\n",
    "This data is returned as pandas DataFrames and passed on to the next stage.\n",
    "\n",
    "### Transform\n",
    "\n",
    "The **transform.py** script cleans and prepares the data:\n",
    "- It handles missing values by filling them with default values.\n",
    "- It corrects invalid data types and ensures proper formatting.\n",
    "- It removes duplicate rows from the datasets.\n",
    "- It also handles specific data cleaning tasks, like correcting negative values in hospital capacity.\n",
    "\n",
    "### Load\n",
    "\n",
    "The **load.py** script creates the necessary tables in the PostgreSQL database and loads the transformed data into the appropriate tables. It uses SQLAlchemy to connect to the PostgreSQL database and pandas to load the DataFrames into the database.\n",
    "\n",
    "### Drop Tables\n",
    "\n",
    "The **drop_tables.py** script drops all tables in the PostgreSQL database to ensure that new data can be loaded without conflicts. It runs a `DROP TABLE` SQL command for each table in the database.\n",
    "\n",
    "### ETL Pipeline\n",
    "\n",
    "The **pipeline.py** file the entire ETL process by calling the `extract()`, `transform()`, and `load()` functions. It also uses the `drop_all_tables()` function to clear the database before starting the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20603a32-dfb4-43b5-9859-4b68ebe76272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe851fae-32dc-47d4-87e9-68f54e61c173",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
